# DocPilot - AI Page Summarization Extension

A production-quality browser extension that provides real-time AI-powered page summarization with streaming responses and follow-up question support.

## Features

- ğŸš€ **Real-time Streaming**: Watch summaries generate token-by-token
- ğŸ§  **Smart Chunking**: Intelligently splits content by headings and structure
- ğŸ” **Semantic Search**: Ask follow-up questions using vector similarity
- ğŸ¯ **Page Classification**: Adapts summarization style to content type (docs, API, blog, README)
- ğŸ’¾ **Vector Storage**: Reuses embeddings for instant follow-up responses
- ğŸ¨ **Beautiful UI**: Clean overlay with Shadow DOM isolation
- ğŸ”’ **Privacy First**: Uses local LLM - no data sent to external APIs

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Browser Extension                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Content Script    â”‚  Background SW    â”‚  Overlay (React)   â”‚
â”‚  - Extract text    â”‚  - Handle clicks  â”‚  - Stream UI       â”‚
â”‚  - Inject Shadow   â”‚  - SSE connection â”‚  - Chat interface  â”‚
â”‚  - Forward events  â”‚  - Route messages â”‚  - Status display  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼ SSE (Server-Sent Events)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FastAPI Backend                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  POST /api/summarize        â”‚  POST /api/followup           â”‚
â”‚  - Streaming SSE response   â”‚  - Vector similarity search   â”‚
â”‚  - Progress events          â”‚  - Context-aware answers      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LangGraph Agent                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  classify_page â†’ chunk_content â†’ embed_store â†’ summarize    â”‚
â”‚       â†“                                           â†“         â”‚
â”‚  Page type detection              Loop until all chunks doneâ”‚
â”‚       â†“                                           â†“         â”‚
â”‚  docs/api/blog/readme                     merge_summary     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼               â–¼               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Ollama  â”‚   â”‚ HuggingFaceâ”‚  â”‚ Pinecone â”‚
        â”‚ (Mistral)â”‚   â”‚ Embeddings â”‚  â”‚  Vectors â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Prerequisites

### Required Software

1. **Python 3.11+**
   ```bash
   python --version  # Should be 3.11 or higher
   ```

2. **Node.js 18+**
   ```bash
   node --version  # Should be 18 or higher
   ```

3. **Ollama** (for local LLM)
   - Download from [ollama.ai](https://ollama.ai)
   - Install and run:
   ```bash
   ollama serve
   ollama pull mistral  # or ollama pull llama3:8b
   ```

4. **Pinecone Account** (free tier works)
   - Sign up at [pinecone.io](https://www.pinecone.io)
   - Get your API key from the dashboard

## Installation

### Backend Setup

1. **Navigate to backend directory:**
   ```bash
   cd backend
   ```

2. **Create virtual environment:**
   ```bash
   python -m venv venv
   
   # Windows
   .\venv\Scripts\activate
   
   # macOS/Linux
   source venv/bin/activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment:**
   ```bash
   cp .env.example .env
   ```
   
   Edit `.env` and add your Pinecone API key:
   ```env
   PINECONE_API_KEY=your-api-key-here
   DEBUG=true
   ```

5. **Start the server:**
   ```bash
   uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
   ```

### Extension Setup

1. **Navigate to extension directory:**
   ```bash
   cd extension
   ```

2. **Install dependencies:**
   ```bash
   npm install
   ```

3. **Build the extension:**
   ```bash
   npm run build
   ```

4. **Load in Chrome:**
   - Open Chrome and go to `chrome://extensions/`
   - Enable "Developer mode" (top right)
   - Click "Load unpacked"
   - Select the `extension/dist` folder

5. **Create icons** (optional):
   - Add PNG icons to `extension/dist/icons/`:
     - `icon16.png` (16x16)
     - `icon32.png` (32x32)
     - `icon48.png` (48x48)
     - `icon128.png` (128x128)

## Usage

1. **Ensure backend is running:**
   ```bash
   cd backend
   uvicorn app.main:app --reload
   ```

2. **Navigate to any webpage** in Chrome

3. **Click the extension icon** in the toolbar

4. **Watch the summary stream** in the overlay panel

5. **Ask follow-up questions** using the chat interface

## API Endpoints

### Health Check
```bash
GET /api/health
```

### Summarize Page (Streaming)
```bash
POST /api/summarize
Content-Type: application/json

{
  "page_url": "https://example.com/docs",
  "page_text": "...",
  "page_title": "Documentation"
}
```

Response: Server-Sent Events stream
```
event: status
data: {"message": "reading page"}

event: token
data: "The"

event: token
data: " documentation"

event: complete
data: {"summary": "..."}
```

### Follow-up Question (Streaming)
```bash
POST /api/followup
Content-Type: application/json

{
  "page_url": "https://example.com/docs",
  "user_query": "How do I configure authentication?"
}
```

## Project Structure

```
DocPilot/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ state.py         # Agent state schema
â”‚   â”‚   â”‚   â”œâ”€â”€ nodes.py         # LangGraph nodes
â”‚   â”‚   â”‚   â”œâ”€â”€ graph.py         # Graph definition
â”‚   â”‚   â”‚   â””â”€â”€ prompts.py       # Prompt templates
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ pinecone_client.py
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â”‚   â”œâ”€â”€ retrieval.py
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py
â”‚   â”‚   â”‚   â””â”€â”€ chunking.py
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ routes.py        # API endpoints
â”‚   â”‚       â””â”€â”€ streaming_utils.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env.example
â”‚
â””â”€â”€ extension/
    â”œâ”€â”€ public/
    â”‚   â”œâ”€â”€ manifest.json
    â”‚   â””â”€â”€ icons/
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ background/
    â”‚   â”‚   â””â”€â”€ background.ts    # Service worker
    â”‚   â”œâ”€â”€ content/
    â”‚   â”‚   â”œâ”€â”€ content_script.ts
    â”‚   â”‚   â””â”€â”€ extractor.ts     # Page text extraction
    â”‚   â”œâ”€â”€ overlay/
    â”‚   â”‚   â”œâ”€â”€ App.tsx
    â”‚   â”‚   â”œâ”€â”€ index.tsx
    â”‚   â”‚   â”œâ”€â”€ components/
    â”‚   â”‚   â”‚   â”œâ”€â”€ Header.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ StatusBar.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ Summary.tsx
    â”‚   â”‚   â”‚   â””â”€â”€ FollowUp.tsx
    â”‚   â”‚   â””â”€â”€ hooks/
    â”‚   â”‚       â””â”€â”€ useStreaming.ts
    â”‚   â”œâ”€â”€ styles/
    â”‚   â”‚   â””â”€â”€ tailwind.css
    â”‚   â””â”€â”€ types/
    â”‚       â””â”€â”€ index.ts
    â”œâ”€â”€ package.json
    â”œâ”€â”€ tsconfig.json
    â”œâ”€â”€ webpack.config.js
    â”œâ”€â”€ tailwind.config.js
    â””â”€â”€ postcss.config.js
```

## Configuration

### Backend Configuration (.env)

| Variable | Description | Default |
|----------|-------------|---------|
| `PINECONE_API_KEY` | Your Pinecone API key | Required |
| `PINECONE_ENVIRONMENT` | Pinecone region | `us-east-1` |
| `PINECONE_INDEX_NAME` | Index name | `docpilot-pages` |
| `OLLAMA_BASE_URL` | Ollama server URL | `http://localhost:11434` |
| `OLLAMA_MODEL` | LLM model to use | `mistral` |
| `CHUNK_SIZE` | Tokens per chunk | `512` |
| `TOP_K_RETRIEVAL` | Results for similarity search | `5` |
| `DEBUG` | Enable debug mode | `false` |

### Supported LLM Models

- `mistral` (recommended, 7B parameters)
- `llama3:8b` (Meta's LLaMA 3)
- `llama2` (Meta's LLaMA 2)
- `codellama` (for code-heavy pages)

## Development

### Backend Development
```bash
cd backend
source venv/bin/activate
uvicorn app.main:app --reload --log-level debug
```

### Extension Development
```bash
cd extension
npm run dev  # Watches for changes and rebuilds
```

### Type Checking
```bash
# Backend
cd backend
mypy app/

# Extension
cd extension
npm run typecheck
```

## Troubleshooting

### "Model not found" error
```bash
# Pull the model first
ollama pull mistral
```

### CORS errors
- Ensure the backend is running on port 8000
- Check that CORS is enabled in FastAPI

### Extension not loading
- Check Chrome developer console for errors
- Ensure manifest.json is valid
- Verify all files are in dist/ folder

### Pinecone connection issues
- Verify API key is correct
- Check Pinecone dashboard for index status
- Ensure region matches your index

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and linting
5. Submit a pull request

## License

MIT License - see LICENSE file for details.

## Acknowledgments

- [LangGraph](https://github.com/langchain-ai/langgraph) for agent orchestration
- [Ollama](https://ollama.ai) for local LLM inference
- [Pinecone](https://pinecone.io) for vector storage
- [Sentence Transformers](https://www.sbert.net/) for embeddings
